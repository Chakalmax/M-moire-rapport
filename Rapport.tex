\documentclass[letterpaper, 12pt]{report}
\usepackage[top = 1.8cm, left = 3cm, right = 3cm ]{geometry}
\usepackage[pdftex]{graphicx}
\usepackage[english]{babel}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{url}
\usepackage{tikz}
\usepackage{float}
\usepackage{listings}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{epigraph}
\usepackage{fancyhdr}
\usepackage{gensymb}
\usepackage{rotating}
\usepackage[english,ruled,vlined]{algorithm2e}
\usepackage{longtable}
\usepackage{newfloat}
\usepackage{enumitem}
\usepackage{amsfonts}

\DeclareFloatingEnvironment[placement={!ht},name=List]{mylist}
\newtheorem{mydef}{Definition}
\newtheorem{myprop}{Property}
\newtheorem{mylemma}{Lemma}

\def\changemargin#1#2{\list{}{\rightmargin#2\leftmargin#1}\item[]}
\let\endchangemargin=\endlist 

\newcommand{\alinea}{
\hspace*{0.5cm}}

\renewcommand*\sfdefault{phv}
\renewcommand*\rmdefault{ppl}

\renewcommand\epigraphflush{flushright}
\renewcommand\epigraphsize{\normalsize}
\setlength\epigraphwidth{0.7\textwidth}

\definecolor{titlepagecolor}{RGB}{255,20,20}

\DeclareFixedFont{\titlefont}{T1}{phv}{\seriesdefault}{n}{0.375in}
  

\makeatletter
\setcounter{secnumdepth}{3} 
%\setcounter{tocdepth}{3} % makes the subsubsection appear in the table of content.
%\@addtoreset{section}{part}
%
%\renewcommand{\partname}{Partie}


% The following code is borrowed from: http://tex.stackexchange.com/a/86310/10898

\newcommand\titlepagedecoration{%
\begin{tikzpicture}[remember picture,overlay,shorten >= -10pt]

\coordinate (aux1) at ([yshift=-50pt]current page.north east);
\coordinate (aux2) at ([yshift=-380pt]current page.north east);
\coordinate (aux3) at ([xshift=-5cm]current page.north east);
\coordinate (aux4) at ([yshift=-130pt]current page.north east);
\coordinate (aux5) at ([yshift=-4cm]current page.north west);
\coordinate (aux6) at ([xshift=4cm]current page.north west);


\begin{scope}[titlepagecolor!40,line width=12pt,rounded corners=12pt]
\draw
  (aux1) -- coordinate (a)
  ++(225:5) --
  ++(-45:5.1) coordinate (b);
\draw[shorten <= -10pt]
  (aux3) --
  (a) --
  (aux1);
\draw[opacity=0.6,titlepagecolor,shorten <= -10pt]
  (b) --
  ++(225:2.2) --
  ++(-45:2.2);
\draw[opacity=0.5,titlepagecolor,shorten <= -15pt]
  (aux5) --
  (aux6);
\end{scope}
\draw[titlepagecolor,line width=8pt,rounded corners=8pt,shorten <= -10pt]
  (aux4) --
  ++(225:0.8) --
  ++(-45:0.8);

\begin{scope}[titlepagecolor!70,line width=6pt,rounded corners=8pt]
\end{scope}
\end{tikzpicture}%
}

\begin{document}
\begin{titlepage}

\noindent


\newgeometry{bottom = 2cm, top = 2.5cm}
\begin{center}
\includegraphics[scale=0.2]{umonslogo}\\
\vspace*{0.7cm}
\includegraphics[scale=0.32]{fs-logo}\\
\vspace*{2.5cm}
\titlefont Mémoire\\~\\{\LARGE  Data Repairing\\}~\\~\\{\large} \par
\end{center}
\vspace*{3.5cm}
\hfill
\begin{minipage}{0.18\linewidth}
  \begin{flushright}
   \rule{0.5pt}{75pt}
  \end{flushright}
\end{minipage}
\begin{minipage}{0.8\linewidth}
\begin{flushleft}
\textsf{\textbf{Project made by:}} Maxime Van Herzeele\\
\textsf{\textbf{Academic Year:}} 2017-2018\\
\textsf{\textbf{Dissertation director:}} Jeff Wijsen\\
%\textsf{\textbf{Rapporteurs}} Pierre Hauweele \& Tom Mens\\
\textsf{\textbf{Section:}} 2$^{nd}$ Master Bloc in ComputerSciences
\end{flushleft}
\end{minipage}
\vspace*{\fill}
\begin{center}
Faculté des Sciences $\bullet$ University of Mons $\bullet$ Place du Parc 20 $\bullet$ B-7000 Mons
\end{center}
\titlepagedecoration
\end{titlepage}

\newgeometry{top = 3cm, left = 2.5cm, right = 2.5cm}

\pagestyle{fancy}
\lhead{Maxime Van Herzeele}
\rhead{MAB2 Computer Sciences}
\cfoot{\thepage}

\pagenumbering{roman} \setcounter{page}{1} 

%\section*{Remerciements} 
%\vspace*{0.8cm}
%\addcontentsline{toc}{section}{acknowledgement} 
%Todo : remerciement
%\newpage

\tableofcontents
\pagebreak
\listoffigures
\listoftables
\pagebreak

\chapter{Introduction}

\pagenumbering{arabic} \setcounter{page}{1} 

\alinea Many institutions and companies collect, store and use a lot of data. These data could be dirty which means they contain erroneous information. An Erroneous information may mislead anyone who wants to use the database. To prevent this problem, data should respect integrity constraints. These constraints are rules in database and any information who doesn't fit them are considered as a dirty data. But these constraints may be imprecise as well, failing to make the difference between good data and dirty data. For this reason, some data aren't identify as violation as they should be and others data are identify as a violation of constraints (dirty data) and they shouldn't be. Both mistakes on data and integrity constraints are a problem for anyone using the database.\\

For example, during my training, i had to work on a project related to a database with such problems. It has a huge impact for a part of my project. The repair of these data is a project for 2018 and i felt it would be interesting to study the data repairing concept.\\

\emph{Data repairing} means recover erroneous data but also repair bad integrity constraints. It would be naive to think we can delete dirty data as we wish. The number of loss would be huge because sometimes there is only one error in a row on a table. Furthermore, integrity constraint can also be dirty which means row deletion could erase some clean data. For this reason we need techniques to repair both data and  constraint  without loosing too much information and without failing to identify dirty data. \\

In this thesis we are going to analyse the \emph{$\theta$-tolerant repair model} as explained in a scientifique article\cite{main}. First of all, we have to re-explain some notions and definition to understand the $\theta$-tolerant model and we also need to present some database we used to illustrate data repairing models. Next we'll present some data repairing models with among them the $\theta$-model. We'll theorically compare them and identify pros and cons for all them.

TO CONTINUE.

\chapter{Integrity constraints}

%In this chapter we'll remind some important notions that we are going to use to explain some data repairing models. We use database following the relational model which was introduced by E.F. Codd \cite{misc1}. We'll also present some database we're going to use to illustrate different notions.

\alinea In this chapter, we'll remind some well know notions that we are going to use further in this thesis. First we'll introduce some database in order to use them as example to explain many properties and definitions. These database  the relational model which was introduced by E.F. Codd \cite{misc1} Then we'll work on constraints and introduce a new kind of constraint called denial constraint. Denial constraints got some property and useful usage for next chapter.

\section{Database}

\alinea In this section we'll present databases we used as example in this thesis. These databases are used to illustrate data repairing models and others notions we'll  define.\\

The first database comes from the main article used as bibliography in this thesis\cite{main}.

\begin{table}[H]
	\centering
	\begin{tabular}{|c|c c c c c c|}
	\hline
	    & Name & BirthDay & Cellphone Number & Year & Income & Tax\\
	\hline
	 t1 & Ayres & 8-8-1984 & 322-573 & 2007 & 21k & 0\\
	 t2 & Ayres & 5-1-1960 & ***-389 & 2007 & 22k & 0 \\
	 t3 & Ayres & 5-1-1960 & 564-389 & 2007 & 22k & 0 \\
	 t4 & Stanley & 13-8-1987 & 868-701 & 2007 & 23k & 3k\\
	 t5 & Stanley & 31-7-1983 & ***-198 & 2007 & 24k & 0\\
	 t6 & Stanley & 31-7-1983 & 930-198 & 2008 & 24k & 0\\
	 t7 & Dustin & 2-12-1985 & 179-924 & 2008 & 25k & 0 \\
	 t8 & Dustin & 5-9-1980 & ***-870 & 2008 & 100k & 21k \\
	 t9 & Dustin & 5-9-1980 & 824-870 & 2009 & 100k & 21k \\
	 t10 & Dustin & 9-4-1984 & 387-215 & 2009 & 150k & 40k \\
	 \hline
	\end{tabular}
	\caption{\label{tableMain} Database of the main article\cite{main}}.
\end{table}

The second database we are going to use comes from a personal experience. In a training, i had to work on a project related to a database with some dirty data. These data can't be used outside the company but we'll try to get the main idea. It's a table named person, who got several basic information on people from Belgium \footnote{Every data in our database are fictional person.}.
\begin{itemize}
\item \textbf{NISS:} The national number of the person. A national number is unique. Usually, a NISS is formated like this\cite{bcss}
	\begin{itemize}
	\item It start with the birthdate of the person in a YY-MM-DD format. Exception are made for stranger(People without Belgian nationality), but for ease we won't consider these cases.
	\item Number 7 to 9 is even for men and odd for female.
	\item Remaining number are the modulo 97 of the 9 first number. 
	\end{itemize}
\item \textbf{LN:} Person's lastname.
\item \textbf{FN:} Person's firstname.
\item \textbf{Birth\_Date:} birthDate in DD-MM-YYYY format.
\item \textbf{Decease\_Date:} Person's date decease.
\item \textbf{Civil\_State:} Person's current civil state(example: single, married, divorced, decease, widow,...)
\item \textbf{City} : The city where the person lives.
\item \textbf{Post\_code} : The postal code of the city.
\item \textbf{Salary} : The salary of the person for one year
\item \textbf{Tax} : The tax amount the person have to pay every year
\item \textbf{Child} : the number of children the person have.
\end{itemize}

\begin{table}[H]
 \footnotesize	
	\centering
	\hspace*{-2cm}\begin{tabular}{|c|c c c c c c c c c c c|}
	\hline
	    & Niss & LM & FN & Birth\_date & Decease\_date & Civil\_state & City & Post\_code& Salary & Tax & Child\\
	\hline
	 t1 & 14050250845 & Dupont & Jean & 14-05-1902 & 18-05-1962 & decease & Ath & 7822 & 25k & 4k & 2\\
	 t2 & 08042910402 & Brel & Jacques & 08-04-1929 & 09-10-1978 & decease & Schaerbeek & 1030 & 100k & 8k & 1\\
	 t3 & 45060710204 & Merckx & Eddy & 07-06-1945 & null & decease & Schaerbeek & 1030 & 125k & 9k & 2\\
	\hline
	 
	 \hline
	\end{tabular}
	\caption{\label{tablePerson} Table Person}.
\end{table}


\newpage

\section{Constraint on database}

\alinea When you want to add rows in a database, you can't put what you want. It would be a problem if it was possible to add non-logical value on some columns of a table. To avoid this kind of problems, we can add rules on a database. Basically a rule works on this way: if the entry row $t_\alpha$ respects some conditions, we can accept the value. Otherwise $t_\alpha$ is not correct and something is wrong with the value of this entry row.\\

On the relational database model used in most of the database, the notion of \emph{functional dependency(DF)} is applied. We can define it as: 

\begin{mydef}
Given a relation R and a set of attributes $X \in R$, a \textbf{functional dependency (FD)} determines another set $Y \in R$ (written X $\rightarrow$ Y) if and only if each X value is associated with one Y value.
\end{mydef} 

In other words, for a dependency $X \rightarrow Y$ means that for a specific value X there's only one possible value for Y. If the FD is respected on a relation R, we say that R satisfy the FD. Let's take some examples on the table \ref{tablePerson}.

\begin{enumerate}
\item \emph{A NISS identify a person.} : This constraint can be describe by a key constraint $Key\{ Niss \}$
\item \emph{Two persons with the same post code lived in the same district.} : The functional dependency for this constraint is $post\_code \rightarrow district$. For a specific value of $post\_code$ there's only one possible value for $district$.
\item \emph{If someone died the march 18$^{th}$ 1962 , his civil status should be equal to decease.} : In this case we need a conditional functional dependency(CFD) which is typically a functional dependency with equality operator on some columns. A functional dependency should work for all records on the table, CFD can hold some conditions on collumns. $[Decease\_date = $'18-05-1962'$] \rightarrow [civil\_status = decease]$
\end{enumerate}
 
\begin{mydef}
A set $\Sigma$ of FD on the relation schema $A$. The relation $R$ satisfy $\Sigma$ noted $R \models \Sigma$ if for each FD in $\Sigma$, R satisfy the FD.
\end{mydef}

However we can't express everything as functional dependencies. Sometimes it fails. For example, if we want to express "Nobody can die before his own birth", in this case we need to compare the birth date and the decease date. Functional dependencies can't work in this case. However, we can use the first-order logic. 
$$\forall t_\alpha \in R, \; \neg(t_\alpha.decease\_date \leq t_\alpha.birth\_date)$$ We can express the others examples in first-order logic.

\begin{enumerate}
\item \emph{A NISS identify a person.}: $\forall t_\alpha,t_\beta \in R$ $\neg(t_\alpha.NISS = t_\beta.NISS)$ (We suppose in this notation, we can't take $t_\alpha = t_\beta$)
\item \emph{Two persons with the same post code lived in the same district.}: $\forall t_\alpha,t_\beta \in R$ $\neg(t_\alpha.post\_code = t_\beta.post\_code \wedge t_\alpha.district \neq t_\beta.district) $
\item \emph{If some got a decease date, his civil status should be equal to decease.}: $\forall t_\alpha \in R$ $\neg(t_\alpha.decease\_date = $'18-05-1962'$ \wedge t_\alpha.civil\_state = decease)$.
\end{enumerate}

On the main paper which this thesis is based, they define a denial constraint as \cite{main}:

\begin{mydef}
Consider a relation scheme $R$ with attributes $attr(R)$. Let predicate space $\mathbb{P}$ be a set of predicate  $P$ in the form  $v_1 \phi v_2$ or $v1 \phi c$ with $v_1,v_2 \in t_x.A$ , $x \in \{\alpha,\beta\}$ , $t_\alpha,t_\beta \in R$, $A \in attr(R)$, $c$ is a constant and $\phi \in \{ =,<,>,\leq, \geq, \neq \}$ is a build-in operator.  A \textbf{Denial Constraint(DC)}:

$$\varphi : t_\alpha,t_\beta,... \in R, \neg(P_1 \wedge ... \wedge P_m)$$

states that for any tuples $t_\alpha,t_\beta,...$ from R, all the predicates $P_i \in pred(\varphi)$,i = 1,...,m should not be true at the same time.
\end{mydef}

In other words, a denial constraint is a first-order logic conjunction of predicates that shouldn't be true all in the same time. So if one of the predicates is false, the data is consider to be clean. \\

\begin{figure}
	\centering
	\includegraphics[scale=1]{img/quadran.png}
	\caption{A denial constraint(DC) can express many type of others constraints}
\end{figure}
A DC can be oversimplified which means a correct data could consider as a violation. Let's take an example on the table \ref{tableMain}. If we take the following constraint:
$$ \varphi : t_\alpha,t_\beta \in R, \neg(t_\alpha.Name = t_\beta.Name \wedge t_\alpha.CP \neq t_\beta.CP)$$
This constraint means that any person with the same \emph{Name} should get the same cellphone number (\emph{CP}), which is incorrect because two different person can get the same name and of course different cellphone number. For example $t_1$ and $t_2$ don't respect this denial constraint. So it's considered as a violation, but we can see it's two different person because they don't have the same age(represented by a birthday data in this case). If we want a correct DC we need to check their \emph{Birthdate}:

\begin{displaymath}
  \begin{split}
    \varphi : t_\alpha,t_\beta \in R &\; \neg(t_\alpha.Name = t_\beta.Name \wedge t_\alpha.CP \neq t_\beta.CP \wedge \\
    & t_\alpha.Birthday = t_\beta.Birthday)
  \end{split}
\end{displaymath}

In the opposite of oversimplified, a DC can be overrefined which means a dirty data could be consider as a clean data. An example could be :

\begin{displaymath}
  \begin{split}
    \varphi : t_\alpha,t_\beta \in R &\; \neg(t_\alpha.Name = t_\beta.Name \wedge t_\alpha.CP \neq t_\beta.CP \wedge \\
    & t_\alpha.Birthday = t_\beta.Birthday \wedge t_\alpha.Year = t_\beta.Year)
  \end{split}
\end{displaymath}

In this case the Year information is not usefull. We don't recognize $t_5$ and $t_8$ as a violation with this constraint. The year information represents the year we took the information and of course we can take the information of the same person on two different year.

\subsection{Some definitions and properties}

In this section we'll present some definitions and properties on the denial constraint. They are going to help us in following chapters.

\subsubsection{Trivial DC}

A DC can be useless and always true. Such DCs shouldn't be present because they never identify any violations. In that case we say that the denial constraint is \emph{trivial}. We'll definie a trivial DC as:

\begin{mydef}
	We say a denial constraint $\varphi$ is \textbf{trivial} if for any instance $I$  of $R$ we have $\varphi \models I$
\end{mydef}

It's quite easy to discover trivial denial constraint with the following property \cite{DCs}:

\begin{myprop}
\label{trivialprop}
 $\forall P_i, P_j$ if $P_i \in Imp(P_j)$, then $\neq(P_i \wedge P_j)$ is a trivial DC.
\end{myprop}

In \cite{main}, they said for $Imp(\phi)$:

\begin{mydef} For any two values a and b, if $a \phi_2 b$ always implies\footnote{any tuples who satisfy $a\phi_2 b$ satisfy $a\phi_1 b$} $a \phi_1 b$, it means $\phi_2 \in Imp(\phi_1)$.
\end{mydef}

With this definition, we see that if $P_i$ is true, $P_j$ is false and vice versa.\\

For example if we have $x = y$ as predicate and we add $x < y$, both the predicates can't be true at the same time. Table \ref{tableImp} show for each $\phi$ the correspondent $Imp(\phi)$.

\begin{table}[H]
	\centering
	\begin{tabular}{|c|c|c|c|c|c|c|}
	\hline
	   $ \phi$ & = & $\neq$ & > & < & $\leq$& $\geq$\\
	   \hline
	   $ \overline{\phi}$ & $\neq$ & = & $\leq$ & $\geq$ & < & >\\
	   \hline
	   $Imp(\phi)$ & = $\geq$,$\leq$ & $\neq$ & >,$\geq$,$\neq$ & <,$\leq$,$\neq$ & $\geq$ & $\leq$ \\
	\hline
	 
	 \hline
	\end{tabular}
	\caption{\label{tableImp} Table of operator, their opposite and their implication}.
\end{table}

If we say for data the table \ref{tableMain}:
$$\varphi : t_\alpha,t_\beta \in R \; \neg(t_\alpha.Tax = t_\beta.Tax \wedge t_\alpha.Tax < t_\beta.Tax)$$ 

It's a trivial by property \ref{trivialprop} It's quite obvious it's impossible that two persons have the same tax rate and one's tax rate is greater than the other. For the rest of this study we won't consider trivial DCs

\subsubsection{Augmentation}

Further in this report, we'll see addition and deletion operations in order to perform data repairing on these constraint. But adding predicates to valid DCs is useless because of the augmentation property \cite{DCs}:

\begin{myprop}
	If $\varphi = \neg (P_1 \wedge P_2 \wedge ... \wedge P_n)$ is a valide DC, then $\varphi ' = \neg(P_1 \wedge P_2 \wedge ... \wedge P_n \wedge Q)$ is also a valide DC
\end{myprop}

It's quite easy to prove this property. If $\varphi$ is valid it means every $t_\alpha \in R$ is accepted by the constraint. If it's accepted, $\forall t_\alpha$ one of the $P_i \in \varphi$ is false. So whatever $Q$ is, $t_\alpha$ will still be accepted.

\subsubsection{Transitivity}
In \cite{DCs} they defined the transitivity of DCs as:

\begin{myprop}
	If $\varphi = \neg (P_1 \wedge P_2 \wedge ... \wedge P_n \wedge Q_1)$ and $\varphi ' = \neg (R_1 \wedge Q_2 \wedge ... \wedge Q_n \wedge Q_2)$ are both valid DCs and $Q_2 \in Imp(\overline{Q_1})$, \\ then $ \varphi '' = \neg(P_1 \wedge ... \wedge P_n \wedge R_1 \wedge ... \wedge R_n)$ is also a valid DC.
\end{myprop}

In other words if two \textbf{valid} DCs, each with predicates that can't be false in the same time, then merging those DCs and removing the two predicates will produce a \textbf{valid} DC.

It's possible to prove that:

\begin{proof}
	$\forall t_\alpha \in R$, we have $\varphi$ and $\varphi '$ verified.
	
\hspace*{0.5cm}	We have $Q_2 \in Imp(\overline{Q_1})$ so $Q_2$ and $Q_1$ can not be true or false at the same time.
	\begin{enumerate}
	 \item if $Q_1$ is false, then $Q_2$ is true . Therefore $\exists R_i such as R_i$ is false (because $\varphi$ should be verified). $\varphi ''$ is verified too because $R_i \in \varphi ''$
	 \item if $Q_1$ is true, then $Q_2$ is false . Therefore $\exists P_i such as P_i$ is false (because $\varphi$ should be verified). $\varphi ''$ is verified too because $P_i \in \varphi ''$
	\end{enumerate}
\end{proof}

\subsubsection{Refinement}
\label{RefinementSection}
In \cite{main} they define the refinement of a DC as :

\begin{mydef}
 $\varphi_2$ is a \textbf{refinement} of $\varphi_1$, denoted by $\varphi_1 \preceq \varphi_2$, if for each $ P$ : $x\phi_1 y \in pred(\varphi_1)$, there exists a $Q : x \phi_2 y \in pred(\varphi_2)$ such that $\phi_1 \in Imp(\phi_2)$
\end{mydef}

\begin{mydef}
 $\Sigma_2$ is a \textbf{refinement} of $\Sigma_1$, denoted by $\Sigma_1 \preceq \Sigma_2$, if for each $ \varphi_2 \in \Sigma_2$, there exists a $\varphi_2 \in \Sigma_1$ such that $\varphi_1 \preceq \varphi_2$
\end{mydef}

As we can see, if you want to change less data, you should refine your DCs with insertion or substitution. For example if our DC is $t_\alpha.Tax \leq t_\beta$ and we change it to $t_\alpha.Tax < t_\beta$ you'll change less data.

\chapter{Data Repairing}

Errors are frequent in database. Because these anomalies can make applications unreliable, some methods detect them but don't repair the detected anomalies. But if you simple filter the dirty data you've detected, applications could still be unreliable. \cite{anodetect} Instead of only detecting errors and delete them, it's better to repair the dirty data.\\

The goal of data repairing is to find a modification $I'$ for I,an instance of $R$, in which all of violation in the constraints $\Sigma$ are eliminated. In other words, we want $I' \models Sigma$ ($I'$ satisfy $\Sigma$). Data repairing process follows the minimum change principle: the data repair $I'$ have to minimize the data repair cost define as \cite{main}:
\begin{mydef}
If $I'$ is a repair for $I$ instance of $R$ by modifying attribute values without any deletion or assertion tuples, the data repair cost is:
$$ \Delta(I,I') = \sum_{t \in I, A \in attr(R)} w(t.A).dist(I(t.A),I'(t.A) $$
where :
\begin{itemize}
	\item $dist(I(t.A),I'(t.A))$ is the distance between two values on cell $t.A$ in $I$ and $I'$.
	\item $w(t.A)$ is the weight of cell $t.A$.
\end{itemize}
\end{mydef}

We can see that the cost can be the number of cell we changed if we put:
$$
dist(I(t.A),I'(t.A)) =
\left\{
	\begin{array}{ll}
	  1 \; if I(t.A) \neq I'(t.A)\;(the\ value\ changed) \\
	  0 \; otherwhise\;(no\ changes\ were\ made)
	\end{array}
\right.
$$
We can put the distance for numerical values on the difference of the two values. For string values we can use the edit distance.\\

The weight $w(t.A)$ can show the trust of the original value in cell which is subjective or simply be a constant if we don't have a lot of knowledge about the data.\\

It's important to notice it's not impossible to don't find any repair $I'$ that can eliminates all the violations. It's possible any values in $dom(A)$ can fit the constraint. In that case, we can use a \emph{fresh variable (fv)} out of the current domain $dom(A)$ in order to extend the domain. This fresh variable is a value that does not satisfy any predicate, we are sure that we can satisfy the DC. (it's satisfy if at least one of the predicates is false).\\

Let's take an example on the table \ref{tableMain}. Let's say our Denial Constraint is the following one:

$$ \varphi : t_\alpha,t_\beta \in R, \neg(t_\alpha.Income > t_\beta.Income \wedge t_\alpha.Tax \leq t_\beta.Tax)$$

In other words, we supposed that if someone get a higher income than another person then he should paid an higher tax every year. We have $ \langle t_1,t_2 \rangle \not\models \varphi $ because $t_1$.Income < $t2.$Income and $t2$.Tax$\ \leq\ t1$.Tax. Same problem with $ \langle t_1,t_3 \rangle$, $ \langle t_1,t_5 \rangle$, ect... you can find all the violation in the figure \ref{BadTax} A repair $I'$ could be the following one:

\begin{table}[H]
	\centering
	\begin{tabular}{|c|c c c c c c c c c c|}
	\hline
	   & $t_1$ & $t_2$ & $t_3$ &$t_4$ &$t_5$ &$t_6$ &$t_7$ &$t_18$ &$t_9$ &$t_10$ \\
	\hline
	 Tax & 0 & \color{red} $fv_1$ & \color{red} $fv_2$& 3k & \color{orange}$fv_3$& \color{orange} $fv_4$& \color{orange} $fv_5$& 21k & 21k & 40k\\
	 \hline
	\end{tabular}
	\caption{\label{tableExample} Example of repair with Tax}.
\end{table}


\begin{figure}
	\centering
	\includegraphics[scale=1]{img/TaxBad}
	\caption{\label{BadTax} All the violation for $\varphi$}
\end{figure}

The reason we put $fv_1$ as Tax value for $t_2$ is because we knew the following things:
\begin{enumerate}

\item $I(t_1.Tax)=0$ so $I(t_2.Tax)>0$ because $I(t_1.Income)<I(t_2.Income)$
\item $I(t_3.Tax)=3$ so $I(t_2.Tax)<3$ because $I(t_2.Income)<I(t_3.Income)$
\item $dom(Tax) = \{0,3k,21k,40k\}$

\end{enumerate}
Because we had no value in the $dom(Tax)$ that would respect 1 and 2, we need to use a fresh variable $fv_1$ out of the $dom(Tax)$. The same  logic can be used to understand why we had to use $fv_2$ to $fv_5$. We could put random value instead but it could be a problem if we insert correct value in the future (These correct values could be see as dirty one).
We only know that 0< $fv_1$,$fv_2$ <3k in order to respect 1 and 2. In the same idea, we have 3k < $fv_3$,$fv_4$,$fv_5$ < 21k.

We can compute the repair cost for $Tax$ in this table. Let's say that:\\

$$
\forall a,b \in dom(A) \ with \ a \neq b.
\left\{
	\begin{array}{ll}
	   dist(a,a)=0\\
	   dist(a,b)=1\\
	   dist(a,fv)=1.5\\
	\end{array}
\right.
$$

When we don't change anything, the distance is obviously equal to zero. $dist(a,fv)$ have to be higher than $dist(a,b)$ otherwise the cost for a non-domain value will be lower than a domain value and we want to avoid fresh variable as much as possible. If we had $dist(a,fv)$ lower than $dist(a,b)$, any repair that uses fresh variable outside the domain would be better and of course it's not a good behavior. In our example, with the value said just before, we can compute a $\Delta(I,I')$ = 7.5

\section{Integrity constraints variations}

We saw earlier that a constraint can be overrefined failing to detect some error or in the opposite a constraint can be oversimplified leading to consider some good data as an error. Because constraints can be inaccurate we need to modify them. We'll consider two types of constraint variance: predicate deletion and in the opposite predicate insertion.\\

When we perform a predicate insertion, some tuples no longer violate the DC. With this variation we can repair a oversimplified constraint but we need to be careful otherwise the DC can be useless. We need to avoid insertion which can lead to a trivial DC or insertion of predicates with obvious constants(like $t_\alpha.Salary$<0 in the table \ref{tablePerson}). \\

An example of trivial DC is a DC $\varphi$ with a predicate $P_i$ : $x \phi_i y$ and we had another predicate $P_j$ : $x \phi_j y$ in the DC with $\overline{\phi_j} \in Imp(\phi_i)$. 

For overrefined DCs, we need to remove some predicates but we need to be careful. If too many predicates are withdraw, we can get an new oversimplified DCs. The more the predicates are deleted, the higher the data repair cost will be as stated in the Lemma1 in \cite{main}. In the other hand the more the predicates are added, the lower the data repair will be. So if you add more predicates than you remove, the changed data will be smaller than removing more predicates and deleting more. The \emph{data repair cost function} take this effect into consideration. It count positively predicate insertion and negatively predicate addition. For $\Sigma '$ a variant of $\Sigma$ in which all $\varphi ' \in \Sigma '$ are obtained by insertion or deletion of predicates for corresponding $\varphi \in \Sigma$, in \cite{main} they define the constraint variation cost:

\begin{mydef}
For a variant $\Sigma '$ of $\Sigma$, the contraint variation cost is defined as

$$\Theta (\Sigma,\Sigma ') = \sum_{\varphi in \Sigma} edit(\varphi,\varphi ')$$

\hspace*{2cm} where $\varphi '$ is a variant of $\phi$ and $edit(\varphi,\varphi ')$ is the corresponding cost.
\end{mydef}

the $edit(\varphi , \varphi ')$ indicating the cost of changing $\varphi$ to $\varphi'$ is defined as:
\begin{mydef}
$$
 edit(\varphi, \varphi') = \sum_{P \in \varphi \\ \varphi'} c(P) + \lambda \sum_{P \in \varphi' \\ \varphi} c(P)$$
 
where $c(P)$ denote the weighted cost of predicate P and $\lambda$ is the weight of a deletion compare to an addition and -1<$\lambda$<0.
\end{mydef}

We don't have to use $\lambda$ =-1 otherwise a deletion followed by an addition would have a cost equal to 0(and it's a bad idea for predicate substitution). For example if we have:
$$ \varphi : t_\alpha,t_\beta \in R, \neg(t_\alpha.Income > t_\beta.Income \wedge t_\alpha.Tax \leq t_\beta.Tax)$$
This DC express the fact that if I get a higher income than someone else, i should pay a higher tax rate. We'll change this DC by deleting $\alpha.Tax \leq t_\beta.Tax$ by $\alpha.Tax < t_\beta.Tax$. It can express the fact that someone with a very low Income could get a Tax equals to 0.
$$ \varphi ' : t_\alpha,t_\beta \in R, \neg(t_\alpha.Income > t_\beta.Income \wedge t_\alpha.Tax < t_\beta.Tax)$$

The constraint variation with $\lambda$ = $\frac{1}{2}$ and $c(P)$ = 0 is : $edit(\varphi, \varphi') = c(t_\alpha.Tax < t_\beta.Tax) + \frac{1}{2} c(\alpha.Tax \leq t_\beta.Tax)$ = $\frac{1}{2}$

With this new constraint, we got less violations. All the violations can be found the figure \ref{GoodTax}. The modifications we made in table \ref{tableExample} changed to:

\begin{table}[H]
	\centering
	\begin{tabular}{|c|c c c c c c c c c c|}
	\hline
	   & $t_1$ & $t_2$ & $t_3$ &$t_4$ &$t_5$ &$t_6$ &$t_7$ &$t_18$ &$t_9$ &$t_10$ \\
	\hline
	 Tax & 0 & 0 & 0 & \textcolor{red}{0} & 0 & 0 & 0 & 21k & 21k & 40k\\
	 \hline
	\end{tabular}
	\caption{\label{tableExample} Example of repair with Tax}.
\end{table}


\begin{figure}
	\centering
	\includegraphics[scale=1]{img/TaxGood}
	\caption{\label{GoodTax} All the violation for $\varphi '$}
\end{figure}

Indeed, a column with only one mistakes is more likely correct than a column with five dirty data. On our new DC, only the ($t_4.Tax$)=3k have to be changed with the value 0.

When we do constraint modification, we should not consider the case where we delete an entire constraint.  We should not remove all the predicates because a DC like $\neg()$ doesn't mean anything. We can't even say if it's always true or always false.

\subsection{Maximal Constraint Variants}

	Every constraint variant $\Sigma'$ with cost $\Theta(\Sigma,\Sigma')$ shouldn't be considered. They're some variation that we're sure they are not better. To perform a prunning of constraints variant that doesn't generate minimum data repair, we'll use the definition of refinement we explained in the previous chapter at the section \ref{RefinementSection}.
	
\begin{mydef}\cite{main}
 We say that a variant $\varphi '$ of a constraint $\varphi$ with $\varphi \preceq \varphi'$ is \textbf{maximal}, if there does not exist another $\varphi ''$ such that $\varphi ' \preceq \varphi ''$ and $edit(\varphi,\varphi'') = edit (\varphi,\varphi')$
\end{mydef}

\begin{myprop}\cite{main}

For any inserted predicate $ P $ : $x \phi y \in pred(\varphi ') \setminus pred(\varphi)$, if $\phi \in \{\leq,\geq,\neq \}$, then $\varphi '$ is not maximal.

\end{myprop}

With this property we see that it's useless to insert every predicate that you're able to construct. We only have to insert predicates with operators {>,<,=} when considering variants of $\varphi$ Let's illustrate that with an example. We'll take two denial constraint on table \ref{tableMain} for this.
$$\varphi_1 : \forall t_\alpha,t_\beta \in R, \neg( t_\alpha.Name = t_\beta.Name \wedge t_\alpha.Income = t_\beta.Income \wedge t_\alpha.CP \neq t_\beta.CP )$$
$$\varphi_2 : \forall t_\alpha,t_\beta \in R, \neg( t_\alpha.Name = t_\beta.Name \wedge t_\alpha.Income \leq t_\beta.Income \wedge t_\alpha.CP \neq t_\beta.CP )$$


We know that $\leq \in Imp(=)$ (see table \ref{tableImp}) for Income so we have $\varphi_2 \preceq \varphi_1$. By the last property we know that $\varphi_2$ is not maximal because it contains $\leq$ operator. In this scenario the data repair cost is 7 for $\varphi_2$ and $\varphi$ will get a data repair cost equal to 3. 

\begin{table}[H]
	\parbox{.45\linewidth}{
	\centering
	\begin{tabular}{|c|c c c|}
	\hline
	    & Name & Cellphone Number & Income\\
	\hline
	 t1 & Ayres & \color{red}564-389 & \color{red}22k\\
	 t2 & Ayres & \color{red}564-389 & 22k\\
	 t3 & Ayres & 564-389 & 22k\\
	 t4 & Stanley &\color{red} 930-198 &\color{red}24k\\
	 t5 & Stanley &\color{red} 930-198 & 24k\\
	 t6 & Stanley & 930-198 & 24k\\
	 t7 & Dustin & \color{red}824-870 & \color{red}100k\\
	 t8 & Dustin & \color{red}824-870 & 100k\\
	 t9 & Dustin & 824-870 & 100k\\
	 t10 & Dustin & \color{red}824-870 & \color{red}100k\\
	 \hline
	\end{tabular}
	\caption{Correction with $\varphi_2$: 7 changes needed only for the collumn CP.}.
	}
	\hfill
	\parbox{.45\linewidth}{
	\centering
	\begin{tabular}{|c|c c c|}
	\hline
	    & Name & Cellphone Number & Income\\
	\hline
	 t1 & Ayres & 322-573 & 21k\\
	 t2 & Ayres & \color{red} 564-389 & 22k\\
	 t3 & Ayres & 564-389 & 22k\\
	 t4 & Stanley & 868-701 &23k\\
	 t5 & Stanley & \color{red} 930-198 & 24k\\
	 t6 & Stanley & 930-198 & 24k\\
	 t7 & Dustin & 179-924 & 25k\\
	 t8 & Dustin & \color{red} 824-870 & 100k\\
	 t9 & Dustin & 824-870 & 100k\\
	 t10 & Dustin & 387-215 & 150k\\
	 \hline
	\end{tabular}
	\caption{Correction with $\varphi_1$ : only 3 changes are needed.}.
	}
\end{table}



We see that the refinement got a better data repair cost. It's not a coincidence because the following lemma exist: \cite{main}

\begin{mylemma}
 Given two constraitn variants $\Sigma_1,\Sigma_2$ of $\Sigma$ such that $\Sigma_2$
 is also a refinement of $\Sigma_1$, have $\Sigma \preceq \Sigma_1 \preceq \Sigma_2$, is always has $\Delta(I,I_1) \geq \Delta(I,I_2)$, where $I_1$ and $I_2$ are the minimum data repairs with regards to $\Sigma_1$ and $\Sigma_2$, respectively.
\end{mylemma}

As a consequence of this Lemma, any non-maximal set of denial constraint $\Sigma$ can be removed from the possibilities of good repair

\subsection{Pruning our candidates}

In this subsection we'll focus on removing the constraint variant $\Sigma '$ that can't generate the minimum data repair. We are going to  consider two bounds of possible minimum data repairs cost for the instance $I$ : the lower bound noted as $\delta_l(\Sigma',I)$ and the upper bound noted $\delta_u(\Sigma,I)$. We consider the following property:

\begin{myprop}
\label{boundRemove}
	For two constraints variants $\Sigma$ and $\Sigma'$ for the instance $I$ of $R$, if $\delta_u(\Sigma,I) < \delta_l(\Sigma',I)$ then $\Sigma'$ can be discarded.
\end{myprop}

Which means the worst bound(upper) of repair for $\Sigma$ is still better than the best bound(lower) of repair for $\Sigma'$.

\subsubsection{Conflict Graph}

Now, we'll introduce the conflict graph which can represent the violations in an instance $I$ of $R$. In the first place, we need to find all the violations and we could get the data repair cost bound from them. We define the violation set as: \cite{main}

\begin{mydef}
 The violation set noted as $viol(I,\varphi) = {\langle t_i,t_j \rangle | \langle t_i,t_j \rangle \not\models }$ is a set of tuple lists that violate $\varphi$. The violation set of $\Sigma$ is $viol(I,\Sigma) = \cup_{\varphi \in \Sigma}viol(I,\varphi)$.
\end{mydef}

\begin{figure}
 \centering
\hspace*{-1.8cm} \includegraphics[scale=1]{img/grapht4.png}
 \caption{\label{grapht4} Conflict hypergraph for $\varphi$}
\end{figure}

With the conflict hypergraph $G$ we can represent the violations in I. For each violation tuples $ \langle t_i,t_j,...\rangle \in viol(I,\varphi)$ there are an edge for $cell(t_i,t_j,...,\varphi)$ in G. A good repairing $I'$ consist in correcting the data base to be able to remove all the edge on the graph. The hypergraph of $I'$ should be empty.

Let's take an example on the table \ref{tableMain} with a denial constraint we already used:

$$\varphi': \forall t_\alpha,t_\beta \in R , \neg(t_\alpha.Income > t_\beta.Income \wedge t_\alpha.Tax < t_\beta.Tax)$$

For this relation the violation set is:
 $$ viol(I,\varphi') = \{ \langle t_5,t_4 \rangle,\langle t_6,t_4 \rangle,\langle t_7,t_4 \rangle \}$$

On our hypergraph, $\langle t_5,t_4 \rangle \in viol(I,\varphi')$ consist of $cell(t_5,t_4;\varphi')$ which is equal to $\{ t_5.income,t_4.Income,t_5.Tax,t_4.Tax\}$. We want to remove a vertex, i.e eliminate a conflict. Let's first introduce some definition and a Lemma: \cite{main}

\begin{mydef}
	We denote $ \min_{a \in dom(A)} dist(I(t.A),a) $ the weight of a vertex t.A, i.e, the minimum cost should be paid to repair t.A.
\end{mydef}

\begin{mydef}
	$\mathbb{V}(G)$ is the \textbf{minimum weighted vertex cover} of the hypergraph G corresponding to $\Sigma$ with weight
	$$||\mathbb{V*(G)}|| = \sum_{t.A \in \mathbb{V}(G)} min_{a\in dom(A)} dist(I(t.A),a)$$
\end{mydef}

\begin{mylemma}
	For any valid repair $I'$ of $I$, i.e, $I' \models \Sigma$, we have $\Delta(I,I') \leq ||V*(G)||$. 
\end{mylemma}

%Computing $\mathbb{V}$ is a NP-Hard problem so we need to approximate it. We'll consider an constant factor-f approximation of $\mathbb{V}$ which is $\frac{||\mathbb{V}(G)||}{||\mathbb{V}*(G)||} \leq f$. where f is the maximum degree of hyperedges and $||\mathbb{V}(G)||$ is an approximation of $||\mathbb{V}*(G)||$.

In \cite{main} they define the upper and lower bound in this way :
$$ \delta_l(\Sigma,I) = \frac{||\mathbb{V}(G)||}{Deg((\Sigma)}$$
$$ \delta_u(\Sigma,I) = \sum_{t.A \in \mathbb{V}(G)} dist(I(t.A),fv)$$

If we come back to our example and suppose that we have :
$$
\forall a,b \in dom(A) \ with \ a \neq b.
\left\{
	\begin{array}{ll}
	   dist(a,a)=0\\
	   dist(a,b)=1\\
	   dist(a,fv)=1.1\\
	\end{array}
\right.
$$

So if each vertex has a weight of 1 (=$ dist(a,b)$) and if we put $\mathbb{V}(G)$ = $\{t_4.Tax \}$ we get $||\mathbb{V}(G)||$ = 1. We also have $Deg(\varpi')$ = 4, so with formula we have we know upper and lower bound : $\delta_l(\Sigma,I)$=$\frac{||\mathbb{V}(G)||}{Deg((\Sigma)}$ = $\frac{1}{4}$ = 0.25 and $\delta_u(\Sigma,I) = \sum_{t.A \in \mathbb{V}(G)} dist(I(t.A),fv)$ = $dist(a,fv)$=1.1 .


\section{$\theta$-tolerant model}

In this section we'll talk about the $\theta$-tolerant repair model which is the main models we want to study. $\theta$ is a threshold on the variation on the set of constraint $\Sigma$, so we don't want a constraint variation cost greater than $\theta$ : $\Theta(\Sigma,\Sigma') \leq \theta$. It helps to avoid any kind of over-refinement and so don't detect some dirty data. To avoid the over-simplification and identify correct data as dirty data, the repairing should pursue the minimum change principle. We need to find a repair $I'$ of the original instance $I$ and minimize $\Delta(I,I')$.\\

Finding the best (minimum) $\theta$-tolerant repair is difficult, it's a NP-hard problem. An NP-hard problem is a class of decision problems which are at least as hard as the hardest problems in NP. What we have to remind of it, it's not possible to resolve it in a polynomial time. Even is the first approach we could made is to get all the constraints variant $\Sigma'$ and then compute $\Theta(\Sigma,\Sigma')$, look if it's lower than $\theta$ and then find the minimum data repair $I'$.\\

We saw that we could replace some value by a fresh variable $fv$. It's better to not turn all of them in a fresh variable mainly because $fv$ is not $dom(A)$ and also a repair like this will never return the optimal repair because we want to minimize the repair cost. \\

Now, consider $\mathbb{D}$ = $\Sigma_1 ' x \Sigma_2' x ... \Sigma_{|\Sigma|}'$ where each $\Sigma_i' \in \mathbb{D}$ is a variant of $\Sigma$ obtained by previous variations. Consider those variations bounded by $\theta$ so we have $\Theta(\Sigma,\Sigma') \leq \theta$. The algorithm \ref{tetha} return the best instance $I_min$ for our set of constraint variation $\Sigma$. The algorithm is simple. For each $\Sigma_i$ , if the lower bound is lower than the previous upper bound (remember the property \ref{boundRemove}). we update the value of $\delta_{min}$ if a better repaired instance comes from $I_i$. \\

\IncMargin{1em}
\begin{algorithm}
\label{tetha}

	\DontPrintSemicolon
  \caption{$\theta$-TolerantRepair$(\mathbb{D},\Sigma,I)$}
  \LinesNumbered
    \SetKwInOut{Input}{Input}
    \SetKwInOut{Output}{Output}

   % \underline{$\theta$-TolerantRepair} $(\mathbb{D},\Sigma,I)$\;
    \Input{Instance $I$, a constraint set $\Sigma$, a set $\mathbb{D}$of constraint variants with variation bound by $\theta$ }
    \Output{A minimum data repair $I_{min}$}
    $\delta_{min}$ = $\delta_u(\Sigma,I)$\;

	\For{each constraint variant $\Sigma_i \in \mathbb{D}$}{
    	\If{$\delta_l (\Sigma_i,I) \leq \delta_{min}$}
    		{
    		$I_i$ = DATAREPAIR($\Sigma_i,I,\mathbb{V}(G_i),\delta_{min}$\;
    		\If{$\Delta(I,I_i) \leq \delta_min$}
    		{$\delta_{min}$ = $\Delta(I,I_i)$\;
    		$I_{min}$ = $I_i$\;}
    		}   		
    		
	}
	\Return $I_{min}$

\end{algorithm}\DecMargin{1em}


To get an example , imagine we have for a $\theta$ = $\frac{1}{2}$ , a set of constraint variation $\mathbb{D}$ = $\{\Sigma_1,\Sigma_2\}$ with the first set of constraints $\Sigma_1$ = $\{\varphi'\}$ and the second set of constraints $\Sigma_2$ = $\{\varphi''\}$ with:
$$\varphi': \forall t_\alpha,t_\beta \in R , \neg(t_\alpha.Income > t_\beta.Income \wedge t_\alpha.Tax < t_\beta.Tax)$$
$$\varphi'': \forall t_\alpha,t_\beta \in R , \neg(t_\alpha.Income > t_\beta.Income \wedge t_\alpha.Tax = t_\beta.Tax)$$

We already have done the conflict hypergraph for $\varphi$ in figure \ref{grapht4} we also know that $\delta(\Sigma_1,I)=1.1$

For $\Sigma2$ we obtain the hypergraph in figure \ref{graphSigma2} (and the violations in figure \ref{EqualTax}) with $\mathbb{V}(G_2)$ = $\{ t_2.Tax,t_3.Tax,t_5.Tax,t_6.Tax,t_7.Tax\}$. We have $Deg(\Sigma_2)$ = $Deg(\varphi')$ \footnote{same reasoning: 4 cells involved.}, so we have $\delta_l(\Sigma_2,I)$= $\frac{6}{2}$ = 1,5. Remember that $\delta_u(\Sigma_1,I)$ =1.1, so we have $\delta_u(\Sigma_1,I) < \delta_l(\Sigma_2,I)$ which means we can ignore $\Sigma2$ and don't call the DATAREPAIR function on it.

\begin{figure}
\centering
\hspace*{-1.8cm} \includegraphics[scale=0.95]{img/graph2}
\caption{\label{graphSigma2}Conflict hypergraph for $\Sigma_2$ with :}
	odd number for Tax, even number for Income. \\
	$t_1$ in yellow, $t_2$ in red, $t_3$ in green,\\
	$t_4$ in blue, $t_5$ in purple and $t_6$ in white.
\end{figure}

\begin{figure}
\centering
\includegraphics[scale=1]{img/TaxEqual}
\caption{\label{EqualTax}All the violations for $\varphi''$}
\end{figure}

Let's talk about the complexity. If we say that $l$ is the maximum number involved in a constraint of $\Sigma$ then we can say that the construction of $G_i$ for each $\Sigma_i \in \mathbb{D}$ cost $O(|I|^l)$. The data repairing algorithm get a complexity in time of $O(|I|^l)$ and the algorithm \ref{tetha} runs in $O(|I|^l|\mathbb{D}|)$ time. Usualy a denial constraint get 2 tuples \cite{main}.

\section{Minimum Data Repair and Violation Free}

After using the $\theta$-tolerant model, we know which constraint set $\Sigma'$ derived from $\Sigma$ we have to use to perform a repairing. But we haven't see how to repair yet. In this section we'll focus on the minimum data repair $I'$ based on the $\Sigma'$. To make this we'll use the violation free to be sure we don't create any violation after correcting one data. For example, if we put $t5.Tax$ to 22, we solved the violation $ \langle t_5,t_4 \rangle$ we had with $\varphi'$ \footnote{remember the $\varphi: \forall t_\alpha,t_\beta \in R , \neg(t_\alpha.Income > t_\beta.Income \wedge t_\alpha.Tax < t_\beta.Tax)$ we used many times}but we introduce a new violation $\langle t_8,t_5 \rangle$.\\

Remember we already said that finding a minimum repairing is NP-hard problem. For this reason we need to make some approximation in order to repair. For the following explanation and definition we'll note $\mathbb{C}$ the selected cells $\mathbb{V}(G)$

\subsection{Suspect identification}

\begin{mydef} \cite{main}
	The suspect set $susp(\mathbb{C},\varphi)$ of a $\varphi$ is a set of tuple lists $\langle t_i,t_j,...:\varphi \rangle$ satisfying all the predicates in $\varphi$ which do not involve cells in$\mathbb{C}$.
\end{mydef}

and they satisfy the suspect condition:

\begin{displaymath}
\begin{split}
sc(t_\alpha,t_\beta,...:\varphi) &= \{I(v_1)\phi c| P: v_1 \phi c \in pred(\varphi),v_1 \in \mathbb\{C\}\}\cup \\
	&\{I(v_1)\phi v_2| P: v_1 \phi v_2 \in pred(\varphi),v_1,v_2 \in \mathbb\{C\}\}
\end{split}
\end{displaymath}

Any violation is of course in the suspect list, which lead to the trivial lemma:

\begin{mylemma}
	For any $\mathbb{C}$, it always has $viol(I,\varphi) \subseteq susp(\mathbb{C},\varphi$
\end{mylemma}

And by this way, if we catch all the suspect, we also get all the violation.\\

To explain it, let's return on the example $\varphi'$ related with the hypergraph at figure \ref{grapht4}. We will change only $t_4.Tax$ as we made in the figure \ref{tableExample}. So we have $\mathbb{C}$ =  $\{ t_4.Tax \}$ and $susp(\mathbb{C},\varphi') = \{\langle t_4 , t_1\rangle , 
\langle t_4 , t_2\rangle,
\langle t_4 , t_3\rangle,
\langle t_5 , t_4\rangle,
\langle t_6 , t_4\rangle,
\langle t_7 , t_4\rangle,
\langle t_8 , t_4\rangle,
\langle t_9 , t_4\rangle,
\langle t_{10} , t_4\rangle \}$ If we get $\langle t_4 , t_1\rangle$ ,   we have $t_4.Income > t_1.Income$ but there is a chance that any change on $t_4.Tax$ leads to a new violation($I'(t_4.Tax)<I(t_1.Tax) $. This is the reason why it's on the suspect list. In the figure \ref{fig3} we have a graph with cells not in $\mathbb{C}$ are circle and cell in $\mathbb{C}$ are squares. Red arrow are predicates that are not respected and blue arrows are respected predicates.

\begin{figure}
	\centering
	\includegraphics[scale=1]{img/fig3}
	\caption{\label{fig3}Suspect condition represented by blue arrows and repair context represented by red arrow (with inverse operator)}
\end{figure}

\subsection{Repair context over suspects}

We can now define a repair context. The repair contest of a suspect tuple is something that makes sure the suspect will not satisfy the predicates declared on $\mathbb{C}$. The reason why we need it is because a denial constraint needs at least one false predicate for every rows of the database. The repair context takes the inverse operator of predicates in $\mathbb{C}$. In \cite{main}, the repair context $rc(t_i,t_j,...:\varphi)$ of a suspect $\langle t_i,t_j,... \rangle $is defined as:

\begin{mydef}
	\begin{displaymath}
	\begin{split}
	rc(t_\alpha,t_\beta,...:\varphi) =
	&\{ I'(v_1)\overline{\phi}c|P : v_i\phi c \in pred(\varphi),v_1 \in \mathbb{C}\}\cup\\
	&\{I'(v_1)\overline{\phi} I'(v_2)|P : v_i\phi v_2 \in pred(\varphi),v_1,v_2 \in \mathbb{C}\}\cup\\
	&\{I'(v_1)\overline{\phi} '(v_2)|P : v_i\phi v_2 \in pred(\varphi),v_1,\in \mathbb{C},v2 \not\in \mathbb{C}\}\cup\\
	&\{I(v_1)\overline{\phi} I'(v_2)|P : v_i\phi v_2 \in pred(\varphi),v_2 \in \mathbb{C},v_1 \not\in \mathbb{C}\}.
	\end{split}
	\end{displaymath}
\end{mydef}

\begin{myprop}
Any assignment that satisfies all the repair contexts forms a valid repair $I'$ without introducing any new violations, i.e, $I' \preceq \Sigma$
\end{myprop}

If we continue with our previous example with $\varphi'$, we have $rc(t_4,t_1:\varphi')$ = $\{I'(t_4.Tax \geq I(t_1.Tax)\}$ , $\geq$ is the inverse operator of $<$, and we only consider predicates of $\varphi'$ with cells from $\mathbb{C}$ which are red arrows on the figure \ref{fig3}. We also have $rc(t_5,t_4:\varphi')$ = $\{I'(t_5.Tax \geq I(t_4.Tax)\}$. With both of these repair constraint we have $0 = t_1.Tax \leq t_4.Tax \leq t_5.Tax =0$ which lead to only one possible value : $t_4.Tax =0$. Remember that previously we put a fresh variable $fv$ instead of 0 (see the table \ref{tableExample} in the previous chapter).\\

We want a repair cost as small as possible, which leads to to minimize the repair cost under repair cost constraint:
\begin{displaymath}
\begin{split}
& \hspace*{1cm} \min \sum_{t_i.A \in \mathbb{C}} dst(I(t_i.A),I'(t_i.A)) \\
 &\ under\ the\ constraint : rc(t_i,t_j,...:\varphi)\ with\ \langle t_i,t_j,... \rangle \in susp(\mathbb{C},\varphi),\varphi \in \Sigma
\end{split}
\end{displaymath}

But it could be possible that we can't assign any value (in our domain) that can fit all the repair context. In these case can't put any value except a fresh variable. If a cell is assigned by a fresh variable $fv$ we can remove every repair context with this cell. The reason is that $fv$ are defined as a way they don't satisfy any kind of predicates which include predicates in repair context. If we can solved our problems i.e we can't found value in dom(A) for a repaired instance $I'$, we'll assign a fresh variable until the problems is solvable. It's better to start by cells with the largest number of appearance in predicates in the repair context. We can say $I'(t.A)$ = $fv$ if $rc(t.A,\Sigma)$ which represent all the repair context declared between a constant or between $t.A$ and $v_i \not\in \mathbb{C}$.\\

If we come back to one of our first example : $\varphi : t_\alpha,t_\beta \in R, \neg(t_\alpha.Income > t_\beta.Income \wedge t_\alpha.Tax \leq t_\beta.Tax)$  For $t_2$ we have $rc(t_2.Tax,\{ \varphi \}) = \{ I'(t_2.Tax) > I(t_1.Tax),I(t_4.Tax) > I'(t_2.Tax),I(t_8.Tax) > I'(t_2.Tax),I(t_9.Tax) > I'(t_2.Tax),I(t_10.Tax) > I'(t_2.Tax) \{$ In the same we we did for $\varphi'$, here we have $0 = I(t_1.Tax)< I'(t_2.Tax) < I(t_4.Tax) = 3k$ and there is no value who respect it in dom(A) = $\{ 0,3k,21k,40 \}$
\section{Other repairing}
\subsection{Holistic data repair}
\subsection{...}

\chapter{Implementation and comparison with others models}
\chapter{Conclusion}

\bibliographystyle{plain}


\bibliography{biblio}

\newpage
\appendix
\end{document}